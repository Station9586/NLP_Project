# -*- coding: utf-8 -*-
"""Word2Vec & Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1NGykgR2hsta6gZVqvRqDizZ1KMYWPf

#### Import package
"""

import numpy as np
import pandas as pd
from google.colab import drive

import warnings
warnings.filterwarnings("ignore")

drive.mount("/content/drive", force_remount=True);

"""#### Load Data"""

Columns = ['target', 'ids', 'date', 'flag', 'user', 'text']
Encoding = 'ISO-8859-1'

df = pd.read_csv('drive/MyDrive/Data/training.1600000.processed.noemoticon.csv', encoding=Encoding, names=Columns)
# df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding=Encoding, names=Columns)

df.head()

print(len(df))

df.info()

df['target'].value_counts()

"""#### Data Preprocessing - 1"""

import seaborn as sns
import matplotlib.pyplot as plt

data = df[['target', 'text']]
data.head()

pd.options.mode.chained_assignment = None
data['target'] = data['target'].replace(4, 1)
data['target'].value_counts()

data_pos = data[data['target'] == 1]
data_neg = data[data['target'] == 0]

dataset = pd.concat([data_pos, data_neg])
print(dataset.shape)

dataset.head()

dataset['text'] = dataset['text'].str.lower()
dataset.head()

stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',
             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',
             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',
             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',
             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',
             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',
             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',
             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',
             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', "shes", 'should', "shouldve",'so', 'some', 'such',
             't', 'than', 'that', "thatll", 'the', 'their', 'theirs', 'them',
             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',
             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',
             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',
             'why', 'will', 'with', 'won', 'y', 'you', "youd","youll", "youre",
             "youve", 'your', 'yours', 'yourself', 'yourselves']

STOPWORDS = set(stopwordlist)

def cleaning (text):
    return ' '.join([word for word in text.split() if word not in STOPWORDS])

dataset['text'] = dataset['text'].apply(cleaning)
dataset['text'].head()

import string

english_punctuations = string.punctuation # 按照標點符號分割句子
punctuations_list = english_punctuations

def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)

dataset['text'] = dataset['text'].apply(remove_punctuations)
dataset['text'].tail()

import re

def remove_repeat(text): # 縮減重複字元
    return re.sub(r'(.)\1+', r'\1\1', text)

dataset['text'] = dataset['text'].apply(remove_repeat)
dataset['text'].tail()

def remove_URLs (data):
    return re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', data)

dataset['text'] = dataset['text'].apply(remove_URLs)
dataset['text'].tail()

def remove_numbers (data):
    return re.sub('[0-9]+', ' ', data)

dataset['text'] = dataset['text'].apply(remove_numbers)
dataset['text'].tail()

"""#### Data Preprocessing - 2"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer('\s+', gaps=True)
dataset['text'] = dataset['text'].apply(tokenizer.tokenize)

dataset['text'].tail()

dataset['text'].head()

import nltk
from functools import lru_cache

st = nltk.PorterStemmer()
stem = lru_cache(maxsize=50000)(st.stem) # 函數結果的緩存，最多緩存50000個結果

def stemming (data): # 對單字進行詞幹提取
    data = [stem(word) for word in data]
    return data

dataset['text'] = dataset['text'].apply(stemming)
dataset['text'].head()

lm = nltk.WordNetLemmatizer()
lemmatize = lru_cache(maxsize=50000)(lm.lemmatize)

nltk.download('wordnet')

def lemmatizing (data): # 對單字進行詞形還原
    data = [lemmatize(word) for word in data]
    return data

dataset['text'] = dataset['text'].apply(lemmatizing)
dataset['text'].head()

dataset.head()

dataset[dataset['target'] == 0]["text"]

words = set()
for data in dataset['text']:
    for word in data:
        words.add(word)

print(len(words))

# from wordcloud import WordCloud

# data_neg = dataset[dataset['target'] == 0]['text'].apply(lambda x: ' '.join(x))

new_data = dataset
new_data['text'] = new_data['text'].apply(lambda x: ' '.join(x)) # 將list轉回成字串
new_data.head()

"""#### Use Word2Vec"""

# Use Word2Vec to convert words to vectors
from gensim.models import Word2Vec

sentences = [row.split() for row in new_data['text']]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

word_vectors = model.wv
X = []

for sentence in sentences:
    vectors = []
    for word in sentence:
        vectors.append(word_vectors[word])
    X.append(vectors)

print(X[0])

X = pad_sequences(X) # 將整數序列填充到相同的長度
y = pd.get_dummies(new_data['target']).values # 將目標變量轉換為二進制矩陣

"""#### Split Data (Word2Vec)"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# validation_size = 240000

# x_validate = x_test[-validation_size:]
# y_validate = y_test[-validation_size:]
# x_test = x_test[:-validation_size]
# y_test = y_test[:-validation_size]

"""#### Word2Vec Model & Train"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout
from keras.optimizers import Adam
from keras.regularizers import L2

embed_dim = 128
LSTM_out = 196

mx_features = 10000

model = Sequential()

# model.add(Embedding(mx_features, embed_dim, input_length = X.shape[1]))
model.add(SpatialDropout1D(0.4, input_shape=(X.shape[1], X.shape[2])))
model.add(LSTM(units=LSTM_out, dropout=0.2))
model.add(Dense(units=10, activation='relu'))
# model.add(Dropout(0.25))
model.add(Dense(2, activation='softmax', kernel_regularizer=L2(0.01)))

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])

print(model.summary())

from keras import callbacks

# 連續3次val_loss沒有改善，就停止訓練
early_stopping = callbacks.EarlyStopping(monitor="val_loss", patience=3, mode="min", restore_best_weights=True)

history = model.fit(x_train, y_train, batch_size = 128, epochs = 5, validation_data = (x_test, y_test), callbacks=[early_stopping])

"""#### Show Classification Report (Word2Vec)"""

from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(x_test)
y_pred = np.argmax(y_pred, axis=1)
y_test_cmp = np.argmax(y_test, axis=1)

print(f"Test Accuracy: {(accuracy_score(y_test_cmp, y_pred)*100):.4f}%\n")
print(classification_report(y_test_cmp, y_pred))

"""#### Train & Test Plot (Word2Vec)

##### Accuracy
"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""##### Loss"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# Draw the ROC curve
from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_test_cmp, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""#### Bert Split Data"""

df = None
data = None
dataset = None
data_pos = None
data_neg = None
words = None

choose_size = 40000
new_data = new_data.sample(choose_size, random_state=42)

from sklearn.model_selection import train_test_split

X = new_data['text']
y = new_data['target']

y = np.argmax(y, axis=1)
y = np.array(y)
# new_data = None

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

X = None
y = None

"""#### Use Pre-train Bert Model & Define Model"""

import tensorflow_hub as hub # Tensorflow hub 社群
import tensorflow_text as text

import tensorflow as tf

tfhub_handle_preprocess = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
tfhub_handle_encoder = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"

def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)

model = build_classifier_model()

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()

epochs = 5
batch_size = 32

init_lr = 3e-5
optimizer = tf.keras.optimizers.Adam(learning_rate=init_lr)

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# 印出模型摘要
print(model.summary())

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min', restore_best_weights=True)

history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=batch_size, callbacks=[early_stopping])

print(history.history)

"""#### Accuracy & Loss Plot (BERT)

##### Accuracy
"""

plt.plot(history.history['binary_accuracy'])
plt.plot(history.history['val_binary_accuracy'])

plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""##### Loss"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""##### ROC Curve"""

# Draw the ROC curve
from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()